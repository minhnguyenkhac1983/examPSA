# üöÄ PSE Implementation - Equilibrium Dynamic Pricing Platform

## Part 1: ü§î Assumptions and Clarifications

### Assumptions Made:

1. **Geographic Scope**: System operates globally with primary focus on major cities
2. **Current Technology**: Momentum Mobility already has basic infrastructure (databases, APIs)
3. **User Scale**: 100M+ MAU with peak traffic during rush hours
4. **Latency Requirements**: P99 < 150ms for pricing API
5. **Data Consistency**: Eventual consistency for market state, strong consistency for quoted prices
6. **Geographic Segmentation**: Using S2 cells or Geohash for geofencing
7. **Pricing Algorithms**: Support for multiple algorithms (linear, exponential, ML-based)

### Clarification Questions (if interactive session):

1. **Business Logic**:
   - Is there a maximum limit for surge multiplier? (e.g., 5x, 10x)
   - How to handle pricing in emergency situations?
   - Do we need support for different vehicle types with different pricing?

2. **Technical Constraints**:
   - Budget for infrastructure and third-party services?
   - Compliance requirements (GDPR, data retention)?
   - Integration with existing systems?

3. **Operational**:
   - Team size and expertise level?
   - Timeline for implementation?
   - Rollout strategy (gradual vs big bang)?

---

## Part 2: üèóÔ∏è High-Level Architecture

### System Diagram:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Mobile Apps   ‚îÇ    ‚îÇ   Driver Apps   ‚îÇ    ‚îÇ  Admin Portal   ‚îÇ
‚îÇ   (React/Flutter)‚îÇ    ‚îÇ   (React/Flutter)‚îÇ    ‚îÇ     (React)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚îÇ                      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      API Gateway          ‚îÇ
                    ‚îÇ   (Load Balancer + Auth)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                         ‚îÇ                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pricing Service‚îÇ    ‚îÇ  Geospatial Service  ‚îÇ    ‚îÇ Analytics Service‚îÇ
‚îÇ  (FastAPI)     ‚îÇ    ‚îÇ     (FastAPI)        ‚îÇ    ‚îÇ   (FastAPI)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                         ‚îÇ                        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Stream Processor        ‚îÇ
                    ‚îÇ    (Apache Flink)         ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                         ‚îÇ                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL   ‚îÇ    ‚îÇ      Redis           ‚îÇ    ‚îÇ    MongoDB      ‚îÇ
‚îÇ  (Spatial DB)  ‚îÇ    ‚îÇ   (Cache + Session)  ‚îÇ    ‚îÇ  (Analytics)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Description:

#### 1. **API Gateway**
- **Function**: Load balancing, authentication, rate limiting
- **Technology**: Nginx + FastAPI
- **Port**: 8000

#### 2. **Pricing Service**
- **Function**: Dynamic pricing calculation, providing price estimates
- **Technology**: FastAPI + Python
- **Port**: 8001

#### 3. **Geospatial Service**
- **Function**: Location data processing, geofencing, S2 cells
- **Technology**: FastAPI + PostGIS
- **Port**: 8003

#### 4. **Stream Processor**
- **Function**: Real-time location event processing, supply/demand calculation
- **Technology**: Apache Flink
- **Port**: 8004

#### 5. **Analytics Service**
- **Function**: Reporting, metrics, business intelligence
- **Technology**: FastAPI + MongoDB
- **Port**: 8002

### Data Flow:

1. **Location Updates**: Driver apps ‚Üí API Gateway ‚Üí Geospatial Service ‚Üí Stream Processor
2. **Price Requests**: Mobile apps ‚Üí API Gateway ‚Üí Pricing Service ‚Üí Redis Cache
3. **Market State**: Stream Processor ‚Üí Redis ‚Üí Pricing Service
4. **Analytics**: All services ‚Üí MongoDB ‚Üí Analytics Service

---

## Part 3: üíæ Data Model and Storage

### Core Entity Schema:

#### 1. **SurgeZone**
```sql
CREATE TABLE surge_zones (
    zone_id VARCHAR(50) PRIMARY KEY,
    zone_name VARCHAR(100) NOT NULL,
    s2_cell_id VARCHAR(20) NOT NULL,
    center_lat DECIMAL(10,8) NOT NULL,
    center_lng DECIMAL(11,8) NOT NULL,
    radius_meters INTEGER NOT NULL,
    base_multiplier DECIMAL(3,2) DEFAULT 1.0,
    max_multiplier DECIMAL(4,2) DEFAULT 5.0,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### 2. **DriverLocationEvent**
```sql
CREATE TABLE driver_location_events (
    event_id UUID PRIMARY KEY,
    driver_id VARCHAR(50) NOT NULL,
    latitude DECIMAL(10,8) NOT NULL,
    longitude DECIMAL(11,8) NOT NULL,
    s2_cell_id VARCHAR(20) NOT NULL,
    zone_id VARCHAR(50) REFERENCES surge_zones(zone_id),
    is_available BOOLEAN NOT NULL,
    vehicle_type VARCHAR(20) NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW()
);
```

#### 3. **SupplyDemandSnapshot**
```sql
CREATE TABLE supply_demand_snapshots (
    snapshot_id UUID PRIMARY KEY,
    zone_id VARCHAR(50) REFERENCES surge_zones(zone_id),
    supply_count INTEGER NOT NULL,
    demand_count INTEGER NOT NULL,
    surge_multiplier DECIMAL(4,2) NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW()
);
```

#### 4. **PricingEventLog**
```sql
CREATE TABLE pricing_event_logs (
    log_id UUID PRIMARY KEY,
    zone_id VARCHAR(50) REFERENCES surge_zones(zone_id),
    old_multiplier DECIMAL(4,2),
    new_multiplier DECIMAL(4,2) NOT NULL,
    trigger_reason VARCHAR(100),
    supply_count INTEGER,
    demand_count INTEGER,
    timestamp TIMESTAMP DEFAULT NOW()
);
```

### Technology Justification:

#### **PostgreSQL + PostGIS**
- **Reason**: Spatial queries, ACID compliance, mature ecosystem
- **Usage**: Core business data, geospatial operations
- **Advantages**: Strong consistency, complex queries, spatial indexing

#### **Redis**
- **Reason**: Ultra-fast caching, pub/sub, session storage
- **Usage**: Real-time pricing cache, session management
- **Advantages**: Sub-millisecond latency, high throughput

#### **MongoDB**
- **Reason**: Flexible schema, horizontal scaling, analytics
- **Usage**: Event logs, analytics data, time-series data
- **Advantages**: Schema flexibility, aggregation pipelines

#### **Apache Kafka**
- **Reason**: High-throughput streaming, durability, replay capability
- **Usage**: Location events, pricing updates, audit logs
- **Advantages**: Fault tolerance, horizontal scaling, ordering guarantees

---

## Part 4: üîå API Design

### REST API Endpoints:

#### 1. **Price Estimate API**
```http
POST /api/v1/pricing/estimate
Content-Type: application/json

{
  "pickup_location": {
    "latitude": 37.7749,
    "longitude": -122.4194
  },
  "dropoff_location": {
    "latitude": 37.7849,
    "longitude": -122.4094
  },
  "vehicle_type": "standard",
  "user_id": "user_123"
}

Response:
{
  "quote_id": "quote_abc123",
  "base_fare": 8.50,
  "surge_multiplier": 1.3,
  "final_fare": 11.05,
  "currency": "USD",
  "quote_valid_until": "2025-01-16T10:30:00Z",
  "estimated_duration": 12,
  "estimated_distance": 2.1,
  "zone_info": {
    "zone_id": "sf_downtown",
    "zone_name": "San Francisco Downtown",
    "current_demand_level": "high"
  }
}
```

#### 2. **Heatmap Data API**
```http
GET /api/v1/pricing/heatmap?bounds=37.7,-122.5,37.8,-122.3&zoom=12

Response:
{
  "zones": [
    {
      "zone_id": "sf_financial",
      "zone_name": "Financial District",
      "center": {"latitude": 37.7749, "longitude": -122.4194},
      "surge_multiplier": 1.8,
      "demand_level": "high",
      "supply_count": 15,
      "demand_count": 25,
      "color": "#ff6b6b"
    }
  ],
  "last_updated": "2025-01-16T10:00:00Z",
  "total_zones": 156
}
```

#### 3. **Driver Heatmap API**
```http
GET /api/v1/driver/heatmap?driver_id=driver_123&radius=5000

Response:
{
  "recommended_zones": [
    {
      "zone_id": "sf_mission",
      "zone_name": "Mission District",
      "distance_meters": 1200,
      "surge_multiplier": 2.1,
      "estimated_earnings_per_hour": 45.50,
      "demand_trend": "increasing"
    }
  ],
  "current_location": {
    "latitude": 37.7599,
    "longitude": -122.4148
  }
}
```

---

## Part 5: üîç Deep Dive into Critical Component

### **Real-Time Geospatial Aggregation**

#### Processing millions of location events:

```python
# Stream Processor with Apache Flink
class LocationEventProcessor:
    def __init__(self):
        self.s2_level = 12  # ~0.1km¬≤ cells
        self.window_size = 30  # seconds
        self.supply_demand_state = {}
    
    def process_location_event(self, event):
        # 1. Convert lat/lng to S2 cell
        s2_cell = s2.lat_lng_to_cell_id(
            event.latitude, 
            event.longitude, 
            self.s2_level
        )
        
        # 2. Update supply/demand state
        if event.is_available:
            self.supply_demand_state[s2_cell]['supply'] += 1
        else:
            self.supply_demand_state[s2_cell]['supply'] -= 1
            
        # 3. Calculate surge multiplier
        multiplier = self.calculate_surge_multiplier(s2_cell)
        
        # 4. Emit pricing update
        self.emit_pricing_update(s2_cell, multiplier)
    
    def calculate_surge_multiplier(self, s2_cell):
        state = self.supply_demand_state[s2_cell]
        supply = state.get('supply', 0)
        demand = state.get('demand', 0)
        
        if supply == 0:
            return 5.0  # Max surge
        
        ratio = demand / supply
        if ratio <= 1.0:
            return 1.0
        elif ratio <= 2.0:
            return 1.0 + (ratio - 1.0) * 0.5
        else:
            return 1.5 + min((ratio - 2.0) * 0.3, 3.5)
```

#### Geospatial indexing:

```python
# S2 Cell implementation
import s2sphere

class GeospatialIndexer:
    def __init__(self):
        self.s2_level = 12
        self.cell_cache = {}
    
    def lat_lng_to_s2_cell(self, lat, lng):
        """Convert lat/lng to S2 cell ID"""
        point = s2sphere.LatLng.from_degrees(lat, lng)
        cell_id = s2sphere.CellId.from_lat_lng(point).parent(self.s2_level)
        return str(cell_id.id())
    
    def get_neighboring_cells(self, cell_id, radius_km=1.0):
        """Get neighboring S2 cells within radius"""
        center_cell = s2sphere.CellId(int(cell_id))
        center_point = center_cell.to_lat_lng()
        
        # Calculate cells within radius
        neighboring_cells = []
        for level in range(8, 16):  # Different granularities
            cell = s2sphere.CellId.from_lat_lng(center_point).parent(level)
            if self.distance_to_cell(cell, center_point) <= radius_km:
                neighboring_cells.append(str(cell.id()))
        
        return neighboring_cells
```

#### Stream processing logic:

```python
# Flink Stream Processing
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

class PricingStreamProcessor:
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.table_env = StreamTableEnvironment.create(self.env)
        
    def setup_pricing_pipeline(self):
        # 1. Source: Kafka location events
        location_events = self.table_env.from_kafka(
            "location_events",
            kafka_config={
                "bootstrap.servers": "kafka:9092",
                "group.id": "pricing-processor"
            }
        )
        
        # 2. Windowed aggregation by S2 cell
        windowed_supply_demand = location_events \
            .window(Tumble.over("30.seconds").on("timestamp").alias("w")) \
            .group_by("s2_cell_id, w") \
            .select(
                "s2_cell_id",
                "w.start as window_start",
                "sum(case when is_available then 1 else 0 end) as supply",
                "count(*) as total_events"
            )
        
        # 3. Calculate surge multipliers
        pricing_updates = windowed_supply_demand \
            .map(self.calculate_surge_multiplier) \
            .filter("surge_multiplier > 1.0")
        
        # 4. Sink: Update Redis cache
        pricing_updates.sink_to_redis("pricing_cache")
        
        return pricing_updates
```

---

#### **‚öôÔ∏è The Pricing Engine Logic**

##### Core Pricing Calculation Logic

```python
class PricingEngine:
    def __init__(self):
        self.config_service = ConfigurationService()
        self.geospatial_service = GeospatialService()
        self.analytics_service = AnalyticsService()
        self.pricing_algorithms = {
            'linear': LinearPricingAlgorithm(),
            'exponential': ExponentialPricingAlgorithm(),
            'ml_based': MLPricingAlgorithm(),
            'adaptive': AdaptivePricingAlgorithm()
        }
    
    async def calculate_final_surge_multiplier(self, zone_id: str, context: dict) -> dict:
        """Calculate final surge multiplier for a zone with geospatial smoothing"""
        
        # 1. Get current supply/demand data
        supply_demand = await self._get_supply_demand_data(zone_id)
        
        # 2. Get zone configuration
        zone_config = await self.config_service.get_zone_config(zone_id)
        
        # 3. Apply geospatial smoothing to avoid sharp price cliffs
        smoothed_data = await self._apply_geospatial_smoothing(zone_id, supply_demand)
        
        # 4. Select appropriate pricing algorithm
        algorithm = self._select_pricing_algorithm(zone_config, context)
        
        # 5. Calculate base multiplier
        base_multiplier = await algorithm.calculate_multiplier(smoothed_data, zone_config)
        
        # 6. Apply business rules and constraints
        final_multiplier = await self._apply_business_rules(base_multiplier, zone_config, context)
        
        # 7. Log pricing decision for analytics
        await self._log_pricing_decision(zone_id, final_multiplier, context)
        
        return {
            'zone_id': zone_id,
            'surge_multiplier': final_multiplier,
            'algorithm_used': algorithm.name,
            'confidence_score': algorithm.confidence_score,
            'timestamp': datetime.utcnow(),
            'context': context
        }
    
    async def _apply_geospatial_smoothing(self, zone_id: str, supply_demand: dict) -> dict:
        """Apply geospatial smoothing to avoid sharp price cliffs between adjacent zones"""
        
        # Get neighboring zones within smoothing radius
        neighboring_zones = await self.geospatial_service.get_neighboring_zones(
            zone_id, 
            radius_km=2.0
        )
        
        # Calculate weighted average considering distance
        smoothed_supply = supply_demand['supply']
        smoothed_demand = supply_demand['demand']
        total_weight = 1.0
        
        for neighbor_zone in neighboring_zones:
            neighbor_data = await self._get_supply_demand_data(neighbor_zone['zone_id'])
            distance_km = neighbor_zone['distance_km']
            
            # Weight decreases with distance (exponential decay)
            weight = math.exp(-distance_km / 1.0)  # 1km decay constant
            
            smoothed_supply += neighbor_data['supply'] * weight
            smoothed_demand += neighbor_data['demand'] * weight
            total_weight += weight
        
        # Normalize by total weight
        smoothed_supply /= total_weight
        smoothed_demand /= total_weight
        
        return {
            'supply': smoothed_supply,
            'demand': smoothed_demand,
            'smoothing_factor': total_weight,
            'neighboring_zones_count': len(neighboring_zones)
        }
    
    def _select_pricing_algorithm(self, zone_config: dict, context: dict) -> PricingAlgorithm:
        """Select appropriate pricing algorithm based on zone characteristics and context"""
        
        # Algorithm selection logic
        if zone_config.get('use_ml_pricing', False) and context.get('ml_available', True):
            return self.pricing_algorithms['ml_based']
        elif zone_config.get('pricing_type') == 'exponential':
            return self.pricing_algorithms['exponential']
        elif zone_config.get('pricing_type') == 'adaptive':
            return self.pricing_algorithms['adaptive']
        else:
            return self.pricing_algorithms['linear']
    
    async def _apply_business_rules(self, base_multiplier: float, zone_config: dict, context: dict) -> float:
        """Apply business rules and constraints to final multiplier"""
        
        # 1. Apply minimum and maximum bounds
        min_multiplier = zone_config.get('min_multiplier', 1.0)
        max_multiplier = zone_config.get('max_multiplier', 5.0)
        
        final_multiplier = max(min_multiplier, min(base_multiplier, max_multiplier))
        
        # 2. Apply time-based adjustments
        current_hour = datetime.utcnow().hour
        if 7 <= current_hour <= 9 or 17 <= current_hour <= 19:  # Rush hours
            final_multiplier *= zone_config.get('rush_hour_multiplier', 1.1)
        
        # 3. Apply weather adjustments
        if context.get('weather_condition') == 'severe':
            final_multiplier *= zone_config.get('weather_multiplier', 1.2)
        
        # 4. Apply event-based adjustments
        if context.get('special_event'):
            final_multiplier *= zone_config.get('event_multiplier', 1.3)
        
        # 5. Apply gradual change limits to prevent price shocks
        last_multiplier = await self._get_last_multiplier(zone_config['zone_id'])
        if last_multiplier:
            max_change = zone_config.get('max_change_per_period', 0.5)
            change_limit = max_change * last_multiplier
            
            if abs(final_multiplier - last_multiplier) > change_limit:
                if final_multiplier > last_multiplier:
                    final_multiplier = last_multiplier + change_limit
                else:
                    final_multiplier = last_multiplier - change_limit
        
        return round(final_multiplier, 2)
```

##### Different Pricing Algorithms

```python
class LinearPricingAlgorithm:
    """Linear pricing algorithm for stable, predictable pricing"""
    
    def __init__(self):
        self.name = "linear"
        self.confidence_score = 0.9
    
    async def calculate_multiplier(self, supply_demand: dict, zone_config: dict) -> float:
        """Calculate multiplier using linear relationship"""
        supply = supply_demand['supply']
        demand = supply_demand['demand']
        
        if supply == 0:
            return zone_config.get('max_multiplier', 5.0)
        
        # Linear relationship: multiplier = 1 + (demand/supply - 1) * factor
        ratio = demand / supply
        if ratio <= 1.0:
            return 1.0
        
        linear_factor = zone_config.get('linear_factor', 0.5)
        multiplier = 1.0 + (ratio - 1.0) * linear_factor
        
        return multiplier

class ExponentialPricingAlgorithm:
    """Exponential pricing algorithm for dynamic surge pricing"""
    
    def __init__(self):
        self.name = "exponential"
        self.confidence_score = 0.85
    
    async def calculate_multiplier(self, supply_demand: dict, zone_config: dict) -> float:
        """Calculate multiplier using exponential relationship"""
        supply = supply_demand['supply']
        demand = supply_demand['demand']
        
        if supply == 0:
            return zone_config.get('max_multiplier', 5.0)
        
        # Exponential relationship for more aggressive pricing
        ratio = demand / supply
        if ratio <= 1.0:
            return 1.0
        
        exponential_factor = zone_config.get('exponential_factor', 0.3)
        multiplier = 1.0 + math.exp((ratio - 1.0) * exponential_factor) - 1.0
        
        return multiplier

class MLPricingAlgorithm:
    """Machine learning-based pricing algorithm"""
    
    def __init__(self):
        self.name = "ml_based"
        self.confidence_score = 0.95
        self.model = self._load_ml_model()
    
    async def calculate_multiplier(self, supply_demand: dict, zone_config: dict) -> float:
        """Calculate multiplier using ML model predictions"""
        
        # Prepare features for ML model
        features = {
            'supply': supply_demand['supply'],
            'demand': supply_demand['demand'],
            'supply_demand_ratio': supply_demand['demand'] / max(supply_demand['supply'], 1),
            'hour_of_day': datetime.utcnow().hour,
            'day_of_week': datetime.utcnow().weekday(),
            'zone_type': zone_config.get('zone_type', 'standard'),
            'historical_avg_multiplier': await self._get_historical_avg(zone_config['zone_id']),
            'weather_score': await self._get_weather_score(),
            'event_score': await self._get_event_score(zone_config['zone_id'])
        }
        
        # Predict optimal multiplier
        predicted_multiplier = self.model.predict(features)
        
        return max(1.0, predicted_multiplier)

class AdaptivePricingAlgorithm:
    """Adaptive pricing algorithm that learns from market response"""
    
    def __init__(self):
        self.name = "adaptive"
        self.confidence_score = 0.8
        self.learning_rate = 0.1
    
    async def calculate_multiplier(self, supply_demand: dict, zone_config: dict) -> float:
        """Calculate multiplier using adaptive learning"""
        
        # Get recent performance metrics
        performance_metrics = await self._get_recent_performance(zone_config['zone_id'])
        
        # Adjust algorithm parameters based on performance
        if performance_metrics['completion_rate'] < 0.8:  # Low completion rate
            # Reduce pricing aggressiveness
            adjustment_factor = 0.9
        elif performance_metrics['completion_rate'] > 0.95:  # High completion rate
            # Increase pricing aggressiveness
            adjustment_factor = 1.1
        else:
            adjustment_factor = 1.0
        
        # Calculate base multiplier using exponential algorithm
        base_algorithm = ExponentialPricingAlgorithm()
        base_multiplier = await base_algorithm.calculate_multiplier(supply_demand, zone_config)
        
        # Apply adaptive adjustment
        adaptive_multiplier = base_multiplier * adjustment_factor
        
        return adaptive_multiplier
```

##### Configuration Service Integration

```python
class ConfigurationService:
    """Service for managing pricing configurations and business rules"""
    
    def __init__(self):
        self.redis_client = Redis()
        self.database = PostgreSQL()
        self.cache_ttl = 300  # 5 minutes
    
    async def get_zone_config(self, zone_id: str) -> dict:
        """Get zone-specific pricing configuration"""
        
        # Try cache first
        cached_config = await self.redis_client.get(f"zone_config:{zone_id}")
        if cached_config:
            return json.loads(cached_config)
        
        # Fetch from database
        config = await self.database.fetch_one("""
            SELECT 
                zone_id,
                zone_name,
                min_multiplier,
                max_multiplier,
                pricing_type,
                linear_factor,
                exponential_factor,
                use_ml_pricing,
                rush_hour_multiplier,
                weather_multiplier,
                event_multiplier,
                max_change_per_period,
                zone_type,
                is_active
            FROM zone_pricing_configs 
            WHERE zone_id = %s AND is_active = true
        """, (zone_id,))
        
        if not config:
            # Return default configuration
            config = self._get_default_zone_config(zone_id)
        
        # Cache the configuration
        await self.redis_client.setex(
            f"zone_config:{zone_id}",
            self.cache_ttl,
            json.dumps(config)
        )
        
        return config
    
    async def update_zone_config(self, zone_id: str, config_updates: dict) -> bool:
        """Update zone configuration"""
        
        # Update database
        success = await self.database.execute("""
            UPDATE zone_pricing_configs 
            SET 
                min_multiplier = %s,
                max_multiplier = %s,
                pricing_type = %s,
                linear_factor = %s,
                exponential_factor = %s,
                use_ml_pricing = %s,
                rush_hour_multiplier = %s,
                weather_multiplier = %s,
                event_multiplier = %s,
                max_change_per_period = %s,
                updated_at = NOW()
            WHERE zone_id = %s
        """, (
            config_updates.get('min_multiplier'),
            config_updates.get('max_multiplier'),
            config_updates.get('pricing_type'),
            config_updates.get('linear_factor'),
            config_updates.get('exponential_factor'),
            config_updates.get('use_ml_pricing'),
            config_updates.get('rush_hour_multiplier'),
            config_updates.get('weather_multiplier'),
            config_updates.get('event_multiplier'),
            config_updates.get('max_change_per_period'),
            zone_id
        ))
        
        if success:
            # Invalidate cache
            await self.redis_client.delete(f"zone_config:{zone_id}")
            
            # Notify other services of configuration change
            await self._notify_config_change(zone_id, config_updates)
        
        return success
    
    async def get_global_pricing_rules(self) -> dict:
        """Get global pricing rules and constraints"""
        
        rules = await self.redis_client.get("global_pricing_rules")
        if rules:
            return json.loads(rules)
        
        # Fetch from database
        rules = await self.database.fetch_one("""
            SELECT 
                max_global_multiplier,
                emergency_multiplier,
                maintenance_mode_multiplier,
                default_algorithm,
                price_change_cooldown_seconds,
                surge_detection_threshold,
                price_stability_window_minutes
            FROM global_pricing_rules 
            WHERE is_active = true
        """)
        
        if not rules:
            rules = self._get_default_global_rules()
        
        # Cache for 1 hour
        await self.redis_client.setex(
            "global_pricing_rules",
            3600,
            json.dumps(rules)
        )
        
        return rules
    
    def _get_default_zone_config(self, zone_id: str) -> dict:
        """Get default zone configuration"""
        return {
            'zone_id': zone_id,
            'min_multiplier': 1.0,
            'max_multiplier': 5.0,
            'pricing_type': 'linear',
            'linear_factor': 0.5,
            'exponential_factor': 0.3,
            'use_ml_pricing': False,
            'rush_hour_multiplier': 1.1,
            'weather_multiplier': 1.2,
            'event_multiplier': 1.3,
            'max_change_per_period': 0.5,
            'zone_type': 'standard',
            'is_active': True
        }
    
    def _get_default_global_rules(self) -> dict:
        """Get default global pricing rules"""
        return {
            'max_global_multiplier': 10.0,
            'emergency_multiplier': 1.0,
            'maintenance_mode_multiplier': 1.0,
            'default_algorithm': 'linear',
            'price_change_cooldown_seconds': 60,
            'surge_detection_threshold': 1.2,
            'price_stability_window_minutes': 5
        }
```

##### Key Features of the Pricing Engine:

1. **üéØ Multi-Algorithm Support**: Linear, exponential, ML-based, and adaptive algorithms
2. **üåç Geospatial Smoothing**: Prevents sharp price cliffs between adjacent zones
3. **‚öôÔ∏è Configuration-Driven**: Dynamic configuration management through Configuration Service
4. **üìä Business Rules Engine**: Comprehensive rule application for different scenarios
5. **üîÑ Adaptive Learning**: ML algorithms that learn from market response
6. **‚ö° Real-time Processing**: Sub-150ms pricing calculations
7. **üõ°Ô∏è Price Stability**: Gradual change limits to prevent price shocks
8. **üìà Performance Monitoring**: Continuous monitoring and optimization

---

#### üöÄ NATs & JetStream Solution (Kafka Alternative)

NATs and JetStream provide a high-performance and simpler streaming solution compared to Kafka, capable of processing millions of messages per second with low latency.

##### NATs & JetStream Architecture

```python
# NATs JetStream Configuration
import asyncio
import nats
from nats.js import JetStreamContext
from nats.js.api import StreamConfig, RetentionPolicy, StorageType
import json
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class LocationEvent:
    event_id: str
    driver_id: str
    latitude: float
    longitude: float
    s2_cell_id: str
    zone_id: str
    is_available: bool
    vehicle_type: str
    timestamp: datetime

@dataclass
class PricingUpdate:
    zone_id: str
    surge_multiplier: float
    supply_count: int
    demand_count: int
    algorithm_used: str
    timestamp: datetime

class NATsStreamManager:
    """NATs JetStream manager for pricing system"""
    
    def __init__(self, nats_url: str = "nats://localhost:4222"):
        self.nats_url = nats_url
        self.nc: Optional[nats.NATS] = None
        self.js: Optional[JetStreamContext] = None
        self.streams_configured = False
    
    async def connect(self):
        """Connect to NATs server"""
        self.nc = await nats.connect(self.nats_url)
        self.js = self.nc.jetstream()
        await self._setup_streams()
    
    async def _setup_streams(self):
        """Setup required streams"""
        
        # 1. Location Events Stream
        await self.js.add_stream(StreamConfig(
            name="LOCATION_EVENTS",
            subjects=["location.*"],
            retention=RetentionPolicy.WORKQUEUE,
            storage=StorageType.FILE,
            max_age=3600,  # 1 hour
            max_msgs=1000000,
            max_bytes=1024 * 1024 * 100,  # 100MB
            replicas=3
        ))
        
        # 2. Pricing Updates Stream
        await self.js.add_stream(StreamConfig(
            name="PRICING_UPDATES",
            subjects=["pricing.*"],
            retention=RetentionPolicy.LIMITS,
            storage=StorageType.FILE,
            max_age=86400,  # 24 hours
            max_msgs=100000,
            max_bytes=1024 * 1024 * 50,  # 50MB
            replicas=3
        ))
        
        # 3. Analytics Events Stream
        await self.js.add_stream(StreamConfig(
            name="ANALYTICS_EVENTS",
            subjects=["analytics.*"],
            retention=RetentionPolicy.LIMITS,
            storage=StorageType.FILE,
            max_age=604800,  # 7 days
            max_msgs=1000000,
            max_bytes=1024 * 1024 * 200,  # 200MB
            replicas=3
        ))
        
        # 4. Configuration Updates Stream
        await self.js.add_stream(StreamConfig(
            name="CONFIG_UPDATES",
            subjects=["config.*"],
            retention=RetentionPolicy.LIMITS,
            storage=StorageType.FILE,
            max_age=2592000,  # 30 days
            max_msgs=10000,
            max_bytes=1024 * 1024 * 10,  # 10MB
            replicas=3
        ))
        
        self.streams_configured = True
        print("‚úÖ NATs JetStream streams configured successfully")
    
    async def publish_location_event(self, event: LocationEvent):
        """Publish location event"""
        subject = f"location.{event.zone_id}"
        data = {
            "event_id": event.event_id,
            "driver_id": event.driver_id,
            "latitude": event.latitude,
            "longitude": event.longitude,
            "s2_cell_id": event.s2_cell_id,
            "zone_id": event.zone_id,
            "is_available": event.is_available,
            "vehicle_type": event.vehicle_type,
            "timestamp": event.timestamp.isoformat()
        }
        
        await self.js.publish(subject, json.dumps(data).encode())
    
    async def publish_pricing_update(self, update: PricingUpdate):
        """Publish pricing update"""
        subject = f"pricing.{update.zone_id}"
        data = {
            "zone_id": update.zone_id,
            "surge_multiplier": update.surge_multiplier,
            "supply_count": update.supply_count,
            "demand_count": update.demand_count,
            "algorithm_used": update.algorithm_used,
            "timestamp": update.timestamp.isoformat()
        }
        
        await self.js.publish(subject, json.dumps(data).encode())
    
    async def subscribe_location_events(self, zone_id: str, callback):
        """Subscribe to location events for specific zone"""
        subject = f"location.{zone_id}"
        
        async def message_handler(msg):
            try:
                data = json.loads(msg.data.decode())
                event = LocationEvent(
                    event_id=data["event_id"],
                    driver_id=data["driver_id"],
                    latitude=data["latitude"],
                    longitude=data["longitude"],
                    s2_cell_id=data["s2_cell_id"],
                    zone_id=data["zone_id"],
                    is_available=data["is_available"],
                    vehicle_type=data["vehicle_type"],
                    timestamp=datetime.fromisoformat(data["timestamp"])
                )
                await callback(event)
                await msg.ack()
            except Exception as e:
                print(f"‚ùå Error processing location event: {e}")
                await msg.nak()
        
        # Durable consumer to ensure no message loss
        await self.js.subscribe(
            subject,
            cb=message_handler,
            durable="location-processor",
            manual_ack=True
        )
    
    async def subscribe_all_location_events(self, callback):
        """Subscribe to all location events"""
        subject = "location.*"
        
        async def message_handler(msg):
            try:
                data = json.loads(msg.data.decode())
                event = LocationEvent(
                    event_id=data["event_id"],
                    driver_id=data["driver_id"],
                    latitude=data["latitude"],
                    longitude=data["longitude"],
                    s2_cell_id=data["s2_cell_id"],
                    zone_id=data["zone_id"],
                    is_available=data["is_available"],
                    vehicle_type=data["vehicle_type"],
                    timestamp=datetime.fromisoformat(data["timestamp"])
                )
                await callback(event)
                await msg.ack()
            except Exception as e:
                print(f"‚ùå Error processing location event: {e}")
                await msg.nak()
        
        await self.js.subscribe(
            subject,
            cb=message_handler,
            durable="global-location-processor",
            manual_ack=True
        )
    
    async def close(self):
        """Close NATs connection"""
        if self.nc:
            await self.nc.close()
```

##### Stream Processor with NATs JetStream

```python
class NATsStreamProcessor:
    """Stream processor using NATs JetStream instead of Apache Flink"""
    
    def __init__(self, nats_manager: NATsStreamManager):
        self.nats_manager = nats_manager
        self.supply_demand_state: Dict[str, Dict] = {}
        self.pricing_engine = PricingEngine()
        self.window_size = 30  # seconds
        self.processing_tasks: List[asyncio.Task] = []
    
    async def start_processing(self):
        """Start stream processing"""
        print("üöÄ Starting NATs Stream Processor...")
        
        # Start processing tasks
        self.processing_tasks = [
            asyncio.create_task(self._process_location_events()),
            asyncio.create_task(self._process_pricing_updates()),
            asyncio.create_task(self._windowed_aggregation()),
            asyncio.create_task(self._health_monitor())
        ]
        
        # Wait for all tasks
        await asyncio.gather(*self.processing_tasks)
    
    async def _process_location_events(self):
        """Process location events from NATs"""
        print("üìç Starting location events processing...")
        
        async def handle_location_event(event: LocationEvent):
            # Update supply/demand state
            zone_id = event.zone_id
            if zone_id not in self.supply_demand_state:
                self.supply_demand_state[zone_id] = {
                    'supply': 0,
                    'demand': 0,
                    'last_updated': datetime.utcnow()
                }
            
            # Update supply count
            if event.is_available:
                self.supply_demand_state[zone_id]['supply'] += 1
            else:
                self.supply_demand_state[zone_id]['supply'] -= 1
            
            # Update timestamp
            self.supply_demand_state[zone_id]['last_updated'] = datetime.utcnow()
            
            # Trigger pricing calculation
            await self._trigger_pricing_calculation(zone_id)
        
        # Subscribe to all location events
        await self.nats_manager.subscribe_all_location_events(handle_location_event)
    
    async def _trigger_pricing_calculation(self, zone_id: str):
        """Trigger pricing calculation for zone"""
        try:
            # Get supply/demand data
            supply_demand = self.supply_demand_state.get(zone_id, {'supply': 0, 'demand': 0})
            
            # Calculate surge multiplier
            result = await self.pricing_engine.calculate_final_surge_multiplier(
                zone_id, 
                {'supply_demand': supply_demand}
            )
            
            # Create pricing update
            pricing_update = PricingUpdate(
                zone_id=zone_id,
                surge_multiplier=result['surge_multiplier'],
                supply_count=supply_demand['supply'],
                demand_count=supply_demand['demand'],
                algorithm_used=result['algorithm_used'],
                timestamp=datetime.utcnow()
            )
            
            # Publish pricing update
            await self.nats_manager.publish_pricing_update(pricing_update)
            
            print(f"üí∞ Pricing update for zone {zone_id}: {result['surge_multiplier']}x")
            
        except Exception as e:
            print(f"‚ùå Error calculating pricing for zone {zone_id}: {e}")
    
    async def _process_pricing_updates(self):
        """Process pricing updates and update cache"""
        print("üí∞ Starting pricing updates processing...")
        
        async def handle_pricing_update(msg):
            try:
                data = json.loads(msg.data.decode())
                
                # Update Redis cache
                await self._update_pricing_cache(data)
                
                # Send to analytics service
                await self._send_to_analytics(data)
                
                await msg.ack()
                
            except Exception as e:
                print(f"‚ùå Error processing pricing update: {e}")
                await msg.nak()
        
        # Subscribe to pricing updates
        await self.nats_manager.js.subscribe(
            "pricing.*",
            cb=handle_pricing_update,
            durable="pricing-cache-updater",
            manual_ack=True
        )
    
    async def _windowed_aggregation(self):
        """Windowed aggregation for supply/demand data"""
        print("‚è∞ Starting windowed aggregation...")
        
        while True:
            try:
                current_time = datetime.utcnow()
                
                # Process zones with data older than window_size
                for zone_id, state in self.supply_demand_state.items():
                    time_diff = (current_time - state['last_updated']).total_seconds()
                    
                    if time_diff > self.window_size:
                        # Reset supply/demand for inactive zone
                        state['supply'] = max(0, state['supply'] - 1)
                        state['demand'] = max(0, state['demand'] - 1)
                        state['last_updated'] = current_time
                        
                        # Trigger pricing recalculation
                        await self._trigger_pricing_calculation(zone_id)
                
                # Wait 10 seconds before checking again
                await asyncio.sleep(10)
                
            except Exception as e:
                print(f"‚ùå Error in windowed aggregation: {e}")
                await asyncio.sleep(5)
    
    async def _update_pricing_cache(self, pricing_data: dict):
        """Update Redis cache with pricing data"""
        import redis.asyncio as redis
        
        redis_client = redis.Redis(host='localhost', port=6379, db=0)
        
        cache_key = f"pricing:{pricing_data['zone_id']}"
        cache_data = {
            'surge_multiplier': pricing_data['surge_multiplier'],
            'supply_count': pricing_data['supply_count'],
            'demand_count': pricing_data['demand_count'],
            'algorithm_used': pricing_data['algorithm_used'],
            'timestamp': pricing_data['timestamp'],
            'ttl': 300  # 5 minutes
        }
        
        await redis_client.setex(
            cache_key,
            300,  # 5 minutes TTL
            json.dumps(cache_data)
        )
        
        await redis_client.close()
    
    async def _send_to_analytics(self, pricing_data: dict):
        """Send pricing data to analytics service"""
        analytics_data = {
            'event_type': 'pricing_update',
            'zone_id': pricing_data['zone_id'],
            'surge_multiplier': pricing_data['surge_multiplier'],
            'supply_count': pricing_data['supply_count'],
            'demand_count': pricing_data['demand_count'],
            'algorithm_used': pricing_data['algorithm_used'],
            'timestamp': pricing_data['timestamp']
        }
        
        await self.nats_manager.js.publish(
            f"analytics.pricing.{pricing_data['zone_id']}",
            json.dumps(analytics_data).encode()
        )
    
    async def _health_monitor(self):
        """Monitor stream processor health"""
        print("üè• Starting health monitor...")
        
        while True:
            try:
                # Check number of zones being processed
                active_zones = len([
                    zone_id for zone_id, state in self.supply_demand_state.items()
                    if (datetime.utcnow() - state['last_updated']).total_seconds() < 60
                ])
                
                print(f"üìä Health Status: {active_zones} zones active")
                
                # Check memory usage
                import psutil
                memory_percent = psutil.virtual_memory().percent
                if memory_percent > 80:
                    print(f"‚ö†Ô∏è Warning: High memory usage ({memory_percent}%)")
                
                await asyncio.sleep(30)  # Check every 30 seconds
                
            except Exception as e:
                print(f"‚ùå Error in health monitor: {e}")
                await asyncio.sleep(10)
    
    async def stop_processing(self):
        """Stop stream processing"""
        print("üõë Stopping NATs Stream Processor...")
        
        for task in self.processing_tasks:
            task.cancel()
        
        await asyncio.gather(*self.processing_tasks, return_exceptions=True)
```

##### NATs Cluster Configuration

```yaml
# docker-compose-nats.yml
version: '3.8'

services:
  nats-1:
    image: nats:2.10-alpine
    container_name: nats-1
    command: [
      "--jetstream",
      "--store_dir=/data",
      "--cluster_name=equilibrium",
      "--server_name=nats-1",
      "--cluster=nats://0.0.0.0:6222",
      "--routes=nats://nats-2:6222,nats://nats-3:6222",
      "--http_port=8222",
      "--port=4222"
    ]
    ports:
      - "4222:4222"
      - "8222:8222"
      - "6222:6222"
    volumes:
      - nats-data-1:/data
    networks:
      - nats-cluster

  nats-2:
    image: nats:2.10-alpine
    container_name: nats-2
    command: [
      "--jetstream",
      "--store_dir=/data",
      "--cluster_name=equilibrium",
      "--server_name=nats-2",
      "--cluster=nats://0.0.0.0:6222",
      "--routes=nats://nats-1:6222,nats://nats-3:6222",
      "--http_port=8222",
      "--port=4222"
    ]
    ports:
      - "4223:4222"
      - "8223:8222"
      - "6223:6222"
    volumes:
      - nats-data-2:/data
    networks:
      - nats-cluster

  nats-3:
    image: nats:2.10-alpine
    container_name: nats-3
    command: [
      "--jetstream",
      "--store_dir=/data",
      "--cluster_name=equilibrium",
      "--server_name=nats-3",
      "--cluster=nats://0.0.0.0:6222",
      "--routes=nats://nats-1:6222,nats://nats-2:6222",
      "--http_port=8222",
      "--port=4222"
    ]
    ports:
      - "4224:4222"
      - "8224:8222"
      - "6224:6222"
    volumes:
      - nats-data-3:/data
    networks:
      - nats-cluster

  nats-monitoring:
    image: synadia/prometheus-nats-exporter:latest
    container_name: nats-monitoring
    command: [
      "-varz",
      "-connz",
      "-routez",
      "-subz",
      "-jsz=all",
      "http://nats-1:8222/varz",
      "http://nats-2:8222/varz",
      "http://nats-3:8222/varz"
    ]
    ports:
      - "7777:7777"
    networks:
      - nats-cluster

volumes:
  nats-data-1:
  nats-data-2:
  nats-data-3:

networks:
  nats-cluster:
    driver: bridge
```

##### Pricing Service with NATs

```python
class NATsPricingService:
    """Pricing Service using NATs instead of Kafka"""
    
    def __init__(self):
        self.nats_manager = NATsStreamManager()
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.pricing_engine = PricingEngine()
    
    async def start_service(self):
        """Start pricing service"""
        await self.nats_manager.connect()
        
        # Subscribe to pricing updates
        await self._subscribe_pricing_updates()
        
        print("‚úÖ NATs Pricing Service started")
    
    async def _subscribe_pricing_updates(self):
        """Subscribe to pricing updates from stream processor"""
        
        async def handle_pricing_update(msg):
            try:
                data = json.loads(msg.data.decode())
                
                # Update cache
                await self._update_cache(data)
                
                # Send real-time update to clients
                await self._broadcast_pricing_update(data)
                
                await msg.ack()
                
            except Exception as e:
                print(f"‚ùå Error processing pricing update: {e}")
                await msg.nak()
        
        await self.nats_manager.js.subscribe(
            "pricing.*",
            cb=handle_pricing_update,
            durable="pricing-service",
            manual_ack=True
        )
    
    async def get_pricing_estimate(self, zone_id: str) -> dict:
        """Get pricing estimate for zone"""
        
        # Try cache first
        cached_data = await self.redis_client.get(f"pricing:{zone_id}")
        
        if cached_data:
            return json.loads(cached_data)
        
        # If no cache, calculate real-time
        supply_demand = await self._get_supply_demand_data(zone_id)
        result = await self.pricing_engine.calculate_final_surge_multiplier(
            zone_id,
            {'supply_demand': supply_demand}
        )
        
        # Cache result
        await self._cache_pricing_result(zone_id, result)
        
        return result
    
    async def _broadcast_pricing_update(self, pricing_data: dict):
        """Broadcast pricing update to all connected clients"""
        
        # Send to WebSocket clients
        websocket_data = {
            'type': 'pricing_update',
            'zone_id': pricing_data['zone_id'],
            'surge_multiplier': pricing_data['surge_multiplier'],
            'timestamp': pricing_data['timestamp']
        }
        
        await self.nats_manager.js.publish(
            "websocket.pricing",
            json.dumps(websocket_data).encode()
        )
```

##### Benefits of NATs & JetStream v2.11

1. **‚ö° High Performance**: Process millions of messages/second with < 1ms latency
2. **üîß Simple**: No Zookeeper needed, just NATs server
3. **üíæ JetStream**: Built-in persistence and replay capability
4. **üåê Clustering**: Easy horizontal scaling
5. **üìä Monitoring**: Built-in monitoring and metrics
6. **üõ°Ô∏è Reliability**: At-least-once delivery guarantee
7. **üí∞ Low Cost**: Fewer resources than Kafka
8. **üöÄ Deployment**: Easy to deploy and maintain
9. **üóúÔ∏è Compression**: S2 compression for 50% space savings
10. **üîí Security**: Built-in encryption support
11. **üìà Latest Tech**: NATs 2.11 with enhanced features

##### Kafka vs NATs v2.11 Comparison

| Feature | Kafka | NATs + JetStream v2.11 |
|---------|-------|-------------------------|
| **Setup** | Complex (needs Zookeeper) | ‚úÖ Simple (single binary) |
| **Memory** | High (JVM overhead) | ‚úÖ Low (Go binary) |
| **Latency** | 5-10ms | ‚úÖ < 1ms |
| **Throughput** | 100K-1M msg/s | ‚úÖ 1M-10M msg/s |
| **Persistence** | Needs configuration | ‚úÖ Built-in JetStream |
| **Monitoring** | External tools needed | ‚úÖ Built-in metrics |
| **Resource** | High | ‚úÖ Low |
| **Compression** | Manual setup | ‚úÖ S2 compression built-in |
| **Encryption** | Complex setup | ‚úÖ Built-in support |
| **Version** | Multiple versions | ‚úÖ Latest v2.11 |

---

### **Failure Handling & Graceful Degradation**

### Comprehensive Failure Management Strategy

#### 1. **Failure Categories & Response Strategies**

**Category 1: Infrastructure Failures**
```python
class InfrastructureFailureHandler:
    def __init__(self):
        self.circuit_breakers = {}
        self.fallback_strategies = {
            'redis': self.redis_fallback,
            'database': self.database_fallback,
            'stream_processor': self.stream_processor_fallback
        }
    
    def handle_redis_failure(self):
        """Handle Redis cluster failure with graceful degradation"""
        # 1. Activate circuit breaker
        self.circuit_breakers['redis'] = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60,
            expected_exception=RedisConnectionError
        )
        
        # 2. Switch to local cache + database fallback
        return {
            'cache_strategy': 'local_memory',
            'fallback_source': 'postgresql',
            'degraded_mode': True,
            'performance_impact': 'latency_increase_50ms'
        }
    
    def handle_database_failure(self):
        """Handle database failure with read-only mode"""
        # 1. Switch to read replicas
        # 2. Enable cached data only mode
        # 3. Disable write operations
        return {
            'mode': 'read_only',
            'data_source': 'cached_data',
            'write_operations': 'disabled',
            'user_notification': 'service_limited'
        }
```

**Category 2: Service Failures**
```python
class ServiceFailureHandler:
    def __init__(self):
        self.health_checks = HealthCheckManager()
        self.load_balancer = LoadBalancer()
        self.service_mesh = ServiceMesh()
    
    def handle_pricing_service_failure(self, failed_instances):
        """Handle pricing service instance failures"""
        # 1. Remove failed instances from load balancer
        self.load_balancer.remove_instances(failed_instances)
        
        # 2. Scale up remaining healthy instances
        self.service_mesh.scale_service('pricing-service', scale_factor=1.5)
        
        # 3. Activate simplified pricing algorithm
        return {
            'pricing_algorithm': 'simplified_linear',
            'cache_ttl': 300,  # 5 minutes
            'fallback_multiplier': 1.2,
            'degraded_features': ['complex_pricing', 'ml_predictions']
        }
    
    def handle_geospatial_service_failure(self):
        """Handle geospatial service failure"""
        # 1. Use cached zone data
        # 2. Fallback to simple lat/lng calculations
        # 3. Disable real-time zone updates
        return {
            'zone_calculation': 'cached_zones',
            'real_time_updates': False,
            'fallback_radius': 1000,  # meters
            'accuracy_degradation': 'acceptable'
        }
```

#### 2. **Circuit Breaker Pattern Implementation**

```python
import asyncio
from enum import Enum
from typing import Callable, Any
import time

class CircuitState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, rejecting requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    def __init__(self, 
                 failure_threshold: int = 5,
                 recovery_timeout: int = 60,
                 expected_exception: Exception = Exception):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
        
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection"""
        
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise CircuitBreakerOpenException("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
            
        except self.expected_exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """Check if enough time has passed to attempt reset"""
        return (time.time() - self.last_failure_time) >= self.recovery_timeout
    
    def _on_success(self):
        """Handle successful call"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        """Handle failed call"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Usage in pricing service
class PricingService:
    def __init__(self):
        self.redis_breaker = CircuitBreaker(
            failure_threshold=3,
            recovery_timeout=30,
            expected_exception=RedisConnectionError
        )
        self.database_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60,
            expected_exception=DatabaseConnectionError
        )
    
    async def get_pricing_data(self, zone_id: str):
        """Get pricing data with circuit breaker protection"""
        try:
            # Try Redis first
            return await self.redis_breaker.call(
                self._get_from_redis, zone_id
            )
        except CircuitBreakerOpenException:
            # Fallback to database
            return await self.database_breaker.call(
                self._get_from_database, zone_id
            )
```

#### 3. **Graceful Degradation Strategies**

**Strategy 1: Progressive Feature Disabling**
```python
class GracefulDegradationManager:
    def __init__(self):
        self.degradation_levels = {
            'level_1': {
                'features_disabled': ['ml_predictions', 'advanced_analytics'],
                'performance_impact': 'minimal',
                'user_experience': 'unaffected'
            },
            'level_2': {
                'features_disabled': ['real_time_updates', 'complex_pricing'],
                'performance_impact': 'moderate',
                'user_experience': 'slightly_degraded'
            },
            'level_3': {
                'features_disabled': ['dynamic_pricing', 'heatmap_data'],
                'performance_impact': 'significant',
                'user_experience': 'basic_functionality_only'
            },
            'level_4': {
                'features_disabled': ['all_advanced_features'],
                'performance_impact': 'maximum',
                'user_experience': 'emergency_mode'
            }
        }
    
    def determine_degradation_level(self, system_health: dict) -> str:
        """Determine appropriate degradation level based on system health"""
        if system_health['redis_health'] < 0.3:
            return 'level_4'
        elif system_health['database_health'] < 0.5:
            return 'level_3'
        elif system_health['stream_processor_health'] < 0.7:
            return 'level_2'
        elif system_health['overall_health'] < 0.9:
            return 'level_1'
        else:
            return 'normal'
    
    def apply_degradation_strategy(self, level: str):
        """Apply degradation strategy for given level"""
        strategy = self.degradation_levels.get(level, {})
        
        # Update service configurations
        self._update_pricing_service_config(strategy)
        self._update_cache_strategies(strategy)
        self._notify_users(strategy)
        
        return strategy
```

**Strategy 2: Fallback Data Sources**
```python
class FallbackDataManager:
    def __init__(self):
        self.data_sources = [
            'redis_cluster',      # Primary
            'local_cache',        # Secondary
            'database_replica',   # Tertiary
            'static_data'         # Emergency
        ]
        self.current_source_index = 0
    
    async def get_pricing_data(self, zone_id: str):
        """Get pricing data with automatic fallback"""
        for i, source in enumerate(self.data_sources[self.current_source_index:]):
            try:
                data = await self._fetch_from_source(source, zone_id)
                if data:
                    # Reset to primary source if successful
                    if i > 0:
                        self.current_source_index = 0
                    return data
            except Exception as e:
                logger.warning(f"Failed to fetch from {source}: {e}")
                continue
        
        # All sources failed, return emergency fallback
        return self._get_emergency_fallback(zone_id)
    
    def _get_emergency_fallback(self, zone_id: str):
        """Emergency fallback with static pricing"""
        return {
            'zone_id': zone_id,
            'surge_multiplier': 1.0,  # No surge pricing
            'base_fare': 8.50,
            'final_fare': 8.50,
            'source': 'emergency_fallback',
            'warning': 'Service operating in limited mode'
        }
```

#### 4. **Error Recovery & Self-Healing**

**Automatic Recovery Mechanisms**
```python
class SelfHealingManager:
    def __init__(self):
        self.recovery_strategies = {
            'redis': self._recover_redis,
            'database': self._recover_database,
            'stream_processor': self._recover_stream_processor
        }
        self.health_monitor = HealthMonitor()
    
    async def monitor_and_recover(self):
        """Continuously monitor services and attempt recovery"""
        while True:
            health_status = await self.health_monitor.check_all_services()
            
            for service, status in health_status.items():
                if status['healthy'] == False:
                    await self._attempt_recovery(service, status)
            
            await asyncio.sleep(30)  # Check every 30 seconds
    
    async def _attempt_recovery(self, service: str, status: dict):
        """Attempt to recover a failed service"""
        recovery_strategy = self.recovery_strategies.get(service)
        if not recovery_strategy:
            return
        
        try:
            # Attempt recovery
            success = await recovery_strategy(status)
            
            if success:
                logger.info(f"Successfully recovered {service}")
                # Notify monitoring systems
                await self._notify_recovery(service)
            else:
                logger.error(f"Failed to recover {service}")
                # Escalate to operations team
                await self._escalate_failure(service, status)
                
        except Exception as e:
            logger.error(f"Recovery attempt failed for {service}: {e}")
    
    async def _recover_redis(self, status: dict):
        """Recover Redis cluster"""
        # 1. Check if it's a temporary network issue
        if await self._check_network_connectivity('redis'):
            # 2. Restart Redis service
            await self._restart_service('redis')
            # 3. Wait for health check
            await asyncio.sleep(10)
            # 4. Verify recovery
            return await self.health_monitor.check_service('redis')
        
        return False
    
    async def _recover_database(self, status: dict):
        """Recover database connection"""
        # 1. Check connection pool
        if status.get('error') == 'connection_pool_exhausted':
            # 2. Reset connection pool
            await self._reset_connection_pool()
            # 3. Verify recovery
            return await self.health_monitor.check_service('database')
        
        return False
```

#### 5. **User Communication & Transparency**

**User Notification System**
```python
class UserNotificationManager:
    def __init__(self):
        self.notification_channels = {
            'mobile_app': MobileNotificationService(),
            'web_app': WebNotificationService(),
            'api_response': APIResponseService()
        }
    
    async def notify_service_degradation(self, level: str, affected_features: list):
        """Notify users about service degradation"""
        message = self._generate_degradation_message(level, affected_features)
        
        # Send to all channels
        for channel, service in self.notification_channels.items():
            try:
                await service.send_notification(message)
            except Exception as e:
                logger.error(f"Failed to send notification via {channel}: {e}")
    
    def _generate_degradation_message(self, level: str, features: list):
        """Generate user-friendly degradation message"""
        messages = {
            'level_1': "We're experiencing minor technical issues. Some advanced features may be temporarily unavailable.",
            'level_2': "We're experiencing technical difficulties. Pricing updates may be delayed.",
            'level_3': "We're experiencing significant technical issues. Basic ride booking is available with standard pricing.",
            'level_4': "We're in emergency mode. Only basic functionality is available. We're working to restore full service."
        }
        
        return {
            'message': messages.get(level, "Service temporarily limited"),
            'affected_features': features,
            'estimated_resolution': self._estimate_resolution_time(level),
            'alternative_options': self._suggest_alternatives(level)
        }
```

#### 6. **Monitoring & Alerting**

**Comprehensive Monitoring Setup**
```python
class FailureMonitoringSystem:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        self.dashboard = MonitoringDashboard()
    
    async def setup_monitoring(self):
        """Setup comprehensive failure monitoring"""
        
        # 1. Service health monitoring
        await self._setup_health_checks()
        
        # 2. Performance monitoring
        await self._setup_performance_monitoring()
        
        # 3. Error tracking
        await self._setup_error_tracking()
        
        # 4. Alert configuration
        await self._setup_alerts()
    
    async def _setup_health_checks(self):
        """Setup health check monitoring"""
        health_checks = {
            'pricing_service': {
                'endpoint': '/health',
                'interval': 30,
                'timeout': 5,
                'critical': True
            },
            'redis_cluster': {
                'command': 'redis-cli ping',
                'interval': 15,
                'timeout': 3,
                'critical': True
            },
            'database': {
                'query': 'SELECT 1',
                'interval': 60,
                'timeout': 10,
                'critical': True
            }
        }
        
        for service, config in health_checks.items():
            await self.metrics_collector.add_health_check(service, config)
    
    async def _setup_alerts(self):
        """Setup alerting rules"""
        alert_rules = [
            {
                'name': 'service_down',
                'condition': 'health_check_failed > 3',
                'severity': 'critical',
                'notification': ['slack', 'email', 'pagerduty']
            },
            {
                'name': 'high_error_rate',
                'condition': 'error_rate > 5%',
                'severity': 'warning',
                'notification': ['slack', 'email']
            },
            {
                'name': 'circuit_breaker_open',
                'condition': 'circuit_breaker_state == "open"',
                'severity': 'warning',
                'notification': ['slack']
            }
        ]
        
        for rule in alert_rules:
            await self.alert_manager.add_rule(rule)
```

#### 7. **Disaster Recovery Plan**

**Complete System Recovery**
```python
class DisasterRecoveryManager:
    def __init__(self):
        self.backup_manager = BackupManager()
        self.infrastructure_manager = InfrastructureManager()
        self.data_recovery = DataRecoveryManager()
    
    async def execute_disaster_recovery(self, disaster_type: str):
        """Execute disaster recovery based on disaster type"""
        
        recovery_plans = {
            'data_center_failure': self._recover_from_datacenter_failure,
            'database_corruption': self._recover_from_database_corruption,
            'complete_system_failure': self._recover_from_complete_failure
        }
        
        recovery_plan = recovery_plans.get(disaster_type)
        if not recovery_plan:
            raise ValueError(f"Unknown disaster type: {disaster_type}")
        
        # Execute recovery plan
        return await recovery_plan()
    
    async def _recover_from_datacenter_failure(self):
        """Recover from datacenter failure"""
        # 1. Activate backup datacenter
        await self.infrastructure_manager.activate_backup_datacenter()
        
        # 2. Restore services from backup
        await self._restore_services_from_backup()
        
        # 3. Sync data from last known good state
        await self.data_recovery.sync_from_backup()
        
        # 4. Verify system health
        health_status = await self._verify_system_health()
        
        return {
            'recovery_status': 'completed',
            'recovery_time': '15_minutes',
            'data_loss': 'minimal',
            'system_health': health_status
        }
    
    async def _recover_from_complete_failure(self):
        """Recover from complete system failure"""
        # 1. Restore infrastructure
        await self.infrastructure_manager.restore_infrastructure()
        
        # 2. Restore databases
        await self.backup_manager.restore_databases()
        
        # 3. Restore application services
        await self._restore_application_services()
        
        # 4. Verify end-to-end functionality
        await self._verify_end_to_end_functionality()
        
        return {
            'recovery_status': 'completed',
            'recovery_time': '45_minutes',
            'data_loss': 'acceptable',
            'system_health': 'fully_operational'
        }
```

### Key Benefits of This Failure Handling Strategy:

1. **üõ°Ô∏è Proactive Protection**: Circuit breakers prevent cascade failures
2. **üîÑ Automatic Recovery**: Self-healing mechanisms reduce manual intervention
3. **üìä Transparent Communication**: Users are informed about service status
4. **‚ö° Graceful Degradation**: System maintains basic functionality during failures
5. **üìà Continuous Monitoring**: Real-time health checks and alerting
6. **üöÄ Fast Recovery**: Automated recovery procedures minimize downtime
7. **üíæ Data Protection**: Multiple backup strategies ensure data safety

---
---

## Part 6: üìä Analytical SQL Queries

### Query to find 5 least efficient surge zones:

```sql
WITH zone_performance AS (
    -- Calculate metrics for each zone in the past month
    SELECT 
        sz.zone_id,
        sz.zone_name,
        AVG(sds.surge_multiplier) as avg_multiplier,
        COUNT(DISTINCT CASE WHEN pel.new_multiplier > pel.old_multiplier 
                           THEN pel.log_id END) as surge_events,
        COUNT(DISTINCT CASE WHEN pel.new_multiplier < pel.old_multiplier 
                           THEN pel.log_id END) as decrease_events
    FROM surge_zones sz
    LEFT JOIN supply_demand_snapshots sds 
        ON sz.zone_id = sds.zone_id 
        AND sds.timestamp >= NOW() - INTERVAL '30 days'
    LEFT JOIN pricing_event_logs pel 
        ON sz.zone_id = pel.zone_id 
        AND pel.timestamp >= NOW() - INTERVAL '30 days'
    WHERE sz.is_active = true
    GROUP BY sz.zone_id, sz.zone_name
),

ride_completion AS (
    -- Calculate ride completion rate
    SELECT 
        zone_id,
        COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_rides,
        COUNT(*) as total_rides,
        ROUND(
            COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*), 
            2
        ) as completion_rate
    FROM ride_requests 
    WHERE created_at >= NOW() - INTERVAL '30 days'
    GROUP BY zone_id
),

efficiency_ranking AS (
    -- Combine metrics and calculate efficiency score
    SELECT 
        zp.zone_id,
        zp.zone_name,
        zp.avg_multiplier,
        COALESCE(rc.completion_rate, 0) as completion_rate,
        zp.surge_events,
        zp.decrease_events,
        -- Efficiency score: high multiplier + low completion = inefficient
        (zp.avg_multiplier * 0.7) + ((100 - COALESCE(rc.completion_rate, 0)) * 0.3) as inefficiency_score
    FROM zone_performance zp
    LEFT JOIN ride_completion rc ON zp.zone_id = rc.zone_id
    WHERE zp.avg_multiplier > 1.2  -- Only consider zones with surge
),

ranked_zones AS (
    -- Rank by inefficiency
    SELECT 
        zone_id,
        zone_name,
        avg_multiplier,
        completion_rate,
        surge_events,
        decrease_events,
        inefficiency_score,
        RANK() OVER (ORDER BY inefficiency_score DESC) as inefficiency_rank
    FROM efficiency_ranking
)

-- Final result: Top 5 least efficient zones
SELECT 
    zone_name,
    ROUND(avg_multiplier, 2) as average_multiplier,
    completion_rate,
    inefficiency_rank,
    surge_events,
    decrease_events,
    ROUND(inefficiency_score, 2) as inefficiency_score
FROM ranked_zones
WHERE inefficiency_rank <= 5
ORDER BY inefficiency_rank;
```

---

## Part 7: üöÄ Scalability and Reliability

### High Availability & Fault Tolerance:

#### 1. **High Availability Architecture**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Load Balancer ‚îÇ    ‚îÇ   Load Balancer ‚îÇ
‚îÇ   (Active)      ‚îÇ    ‚îÇ   (Standby)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                      ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                ‚îÇ                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇPricing ‚îÇ    ‚îÇ  Pricing    ‚îÇ    ‚îÇPricing ‚îÇ
‚îÇService ‚îÇ    ‚îÇ  Service    ‚îÇ    ‚îÇService ‚îÇ
‚îÇNode 1  ‚îÇ    ‚îÇ  Node 2     ‚îÇ    ‚îÇNode 3  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                ‚îÇ                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   Redis Cluster ‚îÇ
            ‚îÇ  (3 Masters +   ‚îÇ
            ‚îÇ   6 Replicas)   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 2. **Fault Tolerance Scenarios**

**Scenario 1: Stream Processor Failure**
```python
class StreamProcessorFailover:
    def __init__(self):
        self.primary_processor = FlinkProcessor("primary")
        self.backup_processor = FlinkProcessor("backup")
        self.health_check_interval = 30
        
    def handle_processor_failure(self):
        # 1. Detect failure via health check
        if not self.primary_processor.is_healthy():
            # 2. Switch to backup processor
            self.backup_processor.start()
            
            # 3. Replay events from Kafka
            self.backup_processor.replay_from_checkpoint()
            
            # 4. Update service discovery
            self.update_service_registry("backup")
            
            # 5. Alert operations team
            self.send_alert("Stream processor failover activated")
```

**Scenario 2: Redis Cache Failure**
```python
class CacheFailover:
    def __init__(self):
        self.redis_cluster = RedisCluster()
        self.fallback_cache = LocalCache()
        self.database_fallback = PostgreSQL()
        
    def get_pricing_data(self, zone_id):
        try:
            # Try Redis cluster first
            return self.redis_cluster.get(f"pricing:{zone_id}")
        except RedisConnectionError:
            # Fallback to local cache
            data = self.fallback_cache.get(zone_id)
            if data:
                return data
            
            # Final fallback to database
            return self.database_fallback.get_latest_pricing(zone_id)
```

#### 3. **Recovery Procedures**

```bash
#!/bin/bash

# 1. Health check all services
check_service_health() {
    services=("pricing-service" "geospatial-service" "stream-processor")
    for service in "${services[@]}"; do
        if ! curl -f http://$service:8000/health; then
            echo "Service $service is down, initiating recovery..."
            recover_service $service
        fi
    done
}

# 2. Service recovery
recover_service() {
    local service=$1
    docker-compose restart $service
    sleep 30
    
    # Verify recovery
    if curl -f http://$service:8000/health; then
        echo "Service $service recovered successfully"
    else
        echo "Service $service recovery failed, escalating..."
        send_alert "Critical service $service recovery failed"
    fi
}
```

### Performance Bottlenecks:

#### 1. **10x Scale Bottlenecks**

**Bottleneck 1: Redis Cache**
- **Current**: 10K requests/second
- **10x Scale**: 100K requests/second
- **Solution**: Redis Cluster with 10+ nodes, read replicas

**Bottleneck 2: Database Connections**
- **Current**: 100 connections
- **10x Scale**: 1000+ connections
- **Solution**: Connection pooling, read replicas, sharding

**Bottleneck 3: Stream Processing**
- **Current**: 1M events/minute
- **10x Scale**: 10M events/minute
- **Solution**: Horizontal scaling, parallel processing

#### 2. **Scaling Solutions**

```yaml
# Kubernetes scaling configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pricing-service
spec:
  replicas: 10
  template:
    spec:
      containers:
      - name: pricing-service
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: pricing-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: pricing-service
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```


## üéØ Implementation Summary

### ‚úÖ Completed:

1. **Assumptions and clarifications** - Defined scope and constraints
2. **High-level architecture** - Microservices architecture with 5 core services
3. **Data model** - PostgreSQL + Redis + MongoDB + Kafka
4. **API design** - RESTful APIs with 3 main endpoints
5. **Component detail** - Real-time geospatial processing with S2 cells
6. **SQL queries** - Complex analytics query with CTEs and window functions
7. **Scalability** - HA architecture with fault tolerance

### üöÄ Key Features:

- **Real-time pricing** with latency < 150ms
- **Geospatial processing** with S2 cell indexing
- **Fault tolerance** with automatic failover
- **Horizontal scaling** with Kubernetes
- **Analytics** with complex SQL queries
- **Multi-language support** with i18n service

### üìä Performance Targets:

- **Throughput**: 10K+ requests/second
- **Latency**: P99 < 150ms
- **Availability**: 99.9% uptime
- **Scale**: 100M+ MAU support
- **Geographic**: 1000+ surge zones

---

## üìû Contact Information

**Project Owner & Developer:**
- **Email**: minh.nguyenkhac1983@gmail.com
- **Phone**: +84 837873388
- **Project**: Equilibrium Dynamic Pricing Platform
- **Copyright**: ¬© 2025 Equilibrium Platform. All rights reserved.
- **AI Support**: This project is enhanced with artificial intelligence technologies.

For technical support, implementation questions, or collaboration inquiries, please contact the project owner directly.

---

